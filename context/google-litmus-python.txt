Directory structure:
└── google-litmus/
    └── worker/
        ├── main.py
        └── util/
            ├── assess.py
            ├── deepeval_eval.py
            ├── docsnsnips.py
            ├── ragas_eval.py
            └── settings.py


================================================
File: worker/util/assess.py
================================================
# Copyright 2024 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This module defines the function ask_llm_against_golden() which calls a large language model (LLM) 
to compare two statements. The function is intended for use in applications that need to assess 
the similarity and consistency of different pieces of text, such as comparing user responses 
to expected answers or evaluating the quality of generated text."""

import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from datetime import datetime
from zoneinfo import ZoneInfo
import json

from util.docsnsnips import cleanup_json, strip_references
from util.settings import settings

print("Initializing assessment module...")

# Initialize Vertex AI with project and location settings
vertexai.init(project=settings.project_id, location=settings.location)

# Load the specified LLM
model = GenerativeModel(settings.ai_default_model)

# Configure generation parameters for the LLM
config = GenerationConfig(
    temperature=0.0,  # Control the randomness of the generated text (0.0 = deterministic)
    top_p=0.8,  # Control the diversity of the generated text
    top_k=38,  # Limit the vocabulary size for generation
    candidate_count=1,  # Generate only one candidate response
    max_output_tokens=1000,  # Set the maximum number of tokens for the generated response
)


def ask_llm_against_golden(statement, golden, prompt):
    """
    Compares a statement against a golden statement using a large language model (LLM).

    This function sends a prompt to the LLM, asking it to compare the given statement
    against a "best-known" (golden) statement. The LLM's response is then parsed to
    extract information about the comparison, such as whether the statements are
    contradictory, equivalent, or share similarities.

    Args:
        statement (str): The statement to be assessed.
        golden (str): The golden response to compare against.
        prompt (str): The prompt to guide the LLM's comparison.

    Returns:
        dict: A dictionary containing the results of the comparison, including:
            - 'answered': Whether the statement is a valid response.
            - 'contradictory': Whether the statement contradicts the golden statement.
            - 'contradictory_explanation': Explanation of the contradiction.
            - 'equivalent': Whether the statement is equivalent to the golden statement.
            - 'equivalent_explanation': Explanation of the equivalence.
            - 'addlinfo': Whether the statement contains additional information.
            - 'addlinfo_explanation': Explanation of the additional information.
            - 'missinginfo': Whether the statement is missing information.
            - 'missinginfo_explanation': Explanation of the missing information.
            - 'similarity': Similarity score between the statements (0-1).
            - 'similarity_explanation': Explanation of the similarity score.
            - 'error': Error message if something goes wrong.
    """

    # Get current time in Berlin timezone
    current_time = datetime.now(tz=ZoneInfo("Europe/Berlin"))

    # Construct the prompt for the LLM
    llm_prompt = f"""
Today is {current_time.strftime('%A, %B %-d %Y')}. The current time is {current_time.strftime('%-H:%M')}.

{prompt}

Here is your task:
Statement: {strip_references(statement)}
Best-known response: {golden}

Comparison result:
"""

    try:
        # Send the prompt to the LLM and get the response
        responses = model.generate_content(
            llm_prompt, stream=False, generation_config=config
        )

        # Check if the LLM finished generating the response successfully
        if responses.candidates[0].finish_reason == 1:
            result = responses.text
        else:
            result = "{'error': 'no response from LLM'}"
    except Exception as exc:
        print(f"Error calling LLM: {exc}")
        result = "{'error': 'exception calling LLM'}"

    # Clean up the LLM response and parse it as JSON
    comparison = cleanup_json(str(result))

    try:
        comparison = json.loads(comparison)
    except:
        print(f"ERROR - llm response parsing failed for {comparison}")
        comparison = {"error": "llm response parsing failed"}

    return comparison


def ask_llm_for_action(mission_description, conversation_history):
    """Asks the LLM for the next action to take in the test mission.

    Args:
        mission_description (str): The description of the overall mission.
        conversation_history (list): A list of previous turns in the conversation.

    Returns:
        dict: A dictionary containing the LLM's suggested action, including:
            - 'request' (dict): Data for the API request to be made.
              Should include 'url', 'method', 'body' (if applicable), and 'headers'.
    """

    # Construct the conversation history string for the prompt
    conversation_string = "\n".join(
        [
            f"{turn.get('role', 'unknown')}: {turn['content']}"
            for turn in conversation_history
        ]
    )

    # Build the prompt for the LLM
    prompt = f"""You are helping to complete a test mission. 
    The mission description is: {mission_description}

    Here is the conversation history:
    {conversation_string}

    What is the next request that should be sent?
    Respond with a JSON object in the following format:
    {{
        "request": "<YOUR-NEXT-INPUT>"
    }}
    """

    # Get the LLM's response
    response = model.generate_content(prompt, generation_config=config)
    llm_output = response.text

    try:
        # Attempt to parse the LLM's response as JSON
        llm_action = cleanup_json(str(llm_output))
        llm_action = json.loads(llm_action)
        return llm_action

    except json.JSONDecodeError:
        print(f"Error: Could not parse LLM response as JSON: {llm_output}")
        return None


def is_mission_done(mission_description, conversation_history):
    """Checks if the test mission has been completed.

    Args:
        mission_description (str): The description of the overall mission.
        conversation_history (list): A list of previous turns in the conversation.

    Returns:
        bool: True if the mission is complete, False otherwise.
    """

    # Construct the conversation history string for the prompt
    conversation_string = "\n".join(
        [
            f"{turn.get('role', 'unknown')}: {turn['content']}"
            for turn in conversation_history
        ]
    )

    prompt = f"""Mission description: {mission_description}

    Conversation history:
    {conversation_string}

    Based on the mission description and the conversation history, has the mission been successfully completed? 
    Answer only with "yes" or "no".
    """

    # Get the LLM's response
    response = model.generate_content(prompt, generation_config=config)
    llm_output = response.text.strip().lower()

    # Check if the LLM's response indicates mission completion
    if llm_output == "yes":
        return True
    elif llm_output == "no":
        return False
    else:
        print(f"Error: Unexpected LLM response: {llm_output}")
        return False


def evaluate_mission(
    mission_description, conversation_history, golden_response, golden_response_prompt
):
    """Evaluates the overall success of the test mission.

    Args:
        mission_description (str): The description of the overall mission.
        conversation_history (list): A list of previous turns in the conversation.
        golden_reponse: What should have been answered
        prompt: Golden response prompt

    Returns:
        dict: A dictionary containing the LLM's assessment of the mission.
    """

    current_time = datetime.now(tz=ZoneInfo("Europe/Berlin"))

    # Construct the conversation history string for the prompt
    conversation_string = "\n".join(
        [
            f"{turn.get('role', 'unknown')}: {turn['content']}"
            for turn in conversation_history
        ]
    )

    prompt = f"""
    
    {golden_response_prompt}

    You are evaluating the success of a test mission. The statement is the Mission description.

    Please provide an overall assessment of the mission's success: 
    - Was the mission completed successfully? 
    - What went well? 
    - What could have been improved?

    Today is {current_time.strftime('%A, %B %-d %Y')}. The current time is {current_time.strftime('%-H:%M')}.

    Statement (Mission description): {mission_description}
    Best-known response: {strip_references(golden_response)}

    Conversation history:
    {conversation_string}

    ALWAYS provide also following JSON object fields in your answer:
    {{
        "overall_success": "(Successful/Partially Successful/Failed)",
        "positive_observations": "List observations here",
        "areas_for_improvement": "List areas for improvement here"
    }}

    Comparison result:
    """

    # Get the LLM's response
    response = model.generate_content(prompt, generation_config=config)
    llm_output = response.text

    try:
        # Attempt to parse the LLM's assessment as JSON
        assessment = cleanup_json(str(llm_output))
        assessment = json.loads(assessment)
        return assessment

    except json.JSONDecodeError:
        print(f"Error: Could not parse LLM assessment as JSON: {llm_output}")
        return {"error": "Could not parse LLM assessment"}


print("Assessment module ready.")


================================================
File: worker/util/deepeval_eval.py
================================================
# Copyright 2024 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This module contains functions for evaluating LLM responses using DeepEval."""

from google.cloud import logging
from deepeval import evaluate as deepeval
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualPrecisionMetric,
    ContextualRecallMetric,
    ContextualRelevancyMetric,
    HallucinationMetric,
    BiasMetric,
    ToxicityMetric,
)
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.test_case import LLMTestCase
from langchain_google_vertexai import ChatVertexAI

from util.settings import settings


# Setup logging
logging_client = logging.Client()
WORKER_LOG_NAME = "litmus-worker-log"
worker_logger = logging_client.logger(WORKER_LOG_NAME)


# --- DeepEval Setup ---
# Base LLM for DeepEval
class GoogleVertexAIDeepEval(DeepEvalBaseLLM):
    """Class to implement Vertex AI for DeepEval"""

    def __init__(self, model):  # pylint: disable=W0231
        self.model = model

    def load_model(self):  # pylint: disable=W0221
        return self.model

    def generate(self, prompt: str) -> str:  # pylint: disable=W0221
        chat_model = self.load_model()
        return chat_model.invoke(prompt).content

    async def a_generate(self, prompt: str) -> str:  # pylint: disable=W0221
        chat_model = self.load_model()
        res = await chat_model.ainvoke(prompt)
        return res.content

    def get_model_name(self):  # pylint: disable=W0236 , W0221
        return "Vertex AI Model"


# Initialise safety filters for vertex model

generation_config = {"temperature": 0.0, "topk": 1}

# Initialize Gemini Pro for DeepEval
gemini_pro = ChatVertexAI(
    model_name="gemini-1.5-pro",
    generation_config=generation_config,
    project=settings.project_id,
    location=settings.location,
    response_validation=False,  # Important since deepeval cannot handle validation errors
)
deepeval_llm = GoogleVertexAIDeepEval(model=gemini_pro)


def deepeval_metric_factory(metric_type: str):
    """Factory function to create DeepEval metrics based on metric_type."""
    metrics = {
        "answer_relevancy": AnswerRelevancyMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "faithfulness": FaithfulnessMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "contextual_precision": ContextualPrecisionMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "contextual_recall": ContextualRecallMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "contextual_relevancy": ContextualRelevancyMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "hallucination": HallucinationMetric(
            threshold=0.5, model=deepeval_llm, async_mode=False
        ),
        "bias": BiasMetric(threshold=0.5, model=deepeval_llm, async_mode=False),
        "toxicity": ToxicityMetric(threshold=0.5, model=deepeval_llm, async_mode=False),
    }
    if metric_type not in metrics:
        raise ValueError(f"Invalid metric type: {metric_type}")
    return metrics[metric_type]


def evaluate_deepeval(question, answer, golden_response, context, deepeval_metric):
    """Evaluates the LLM response using DeepEval.

    Args:
        actual_filtered_response (dict): The filtered response from the LLM.
        output_field (str): The key for the output field in the filtered response.
        golden_response (str): The expected (golden) response.

    Returns:
        dict or str: A dictionary containing the DeepEval evaluation results if successful,
                     or an error message if an exception occurs during evaluation.
    """
    try:
        # Create a test case
        deepeval_test_case = LLMTestCase(
            input=question,
            actual_output=answer,
            expected_output=golden_response,  # Include golden response for reference metrics
            retrieval_context=[
                context
            ],  # You might need to provide context here based on your setup
        )

        # Evaluate with DeepEval and store results
        deepeval_metric.measure(deepeval_test_case)
        return {
            "metric": deepeval_metric.__class__.__name__,  # Get class name as metric name
            "score": deepeval_metric.score,
            "reason": deepeval_metric.reason,
        }
    except Exception as e:
        worker_logger.log_text(
            f"Error in DeepEval evaluation: {str(e)}", severity="ERROR"
        )
        return f"Error during DeepEval evaluation: {str(e)}"


================================================
File: worker/util/docsnsnips.py
================================================
# Copyright 2024 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from vertexai.language_models._language_models import GroundingCitation
import re
import datetime


def cleanup_json(x: str):
    """Takes a string with supposed JSON content and eliminates excess characters at beginning and end.

    Args:
        x (str): String containing a JSON structure. May contain leading and trailing characters outside of JSON.

    Returns:
        str: Returns the substring representing the JSON structure. Returns None if JSON could not be found.
    """
    y = x.replace("'", "")
    startBrace = y.find("{")
    if startBrace == -1:
        return None
    endBrace = y.rfind("}")
    if endBrace == -1:
        return None
    if startBrace > endBrace:
        return None
    return y[startBrace : endBrace + 1].replace("\n", " ")


def strip_references(statement: str):
    """Strips reference notation elements (e.g., [3]) from a string.

    Args:
        statement (str): The string that could contain references

    Returns:
        str: The string with any references removed.
    """
    out = ""
    p = 0
    while True:
        if p == len(statement):
            break
        q = statement.find("[", p)
        out += statement[p:q]
        if q < 0:
            break
        else:
            v = statement.find("]", q + 1)
            if v < 0:
                break
            p = v + 1
    return out


def find_citations(text_with_citations) -> list[tuple]:
    """Finds all citations in the text and returns their locations and values.

    Args:
        text_with_citations (str): Text potentially containing citations in the form [number].

    Returns:
        list[tuple]: A list of tuples, each representing a citation.
                     Each tuple contains:
                         - start position (int): The starting index of the citation in the text.
                         - end position (int): The ending index of the citation in the text.
                         - citation value (int): The numerical value of the citation.
    """

    # Find all citations in the text
    citations = []
    p = 0
    q = -1
    while True:
        # Find a citation
        p = text_with_citations.find("[", q + 1)
        if p < 0:  # no (more) citations
            break
        # Find the end of this particular citation
        q = text_with_citations.find("]", p)
        if q < 0:  # In case we have a missing closing bracket
            break
        # Now iterate
        p += 1
        while p < q:
            r = p
            if text_with_citations[r].isdigit():
                while r <= q and text_with_citations[r].isdigit():
                    r += 1
                citations.append((p, r, int(text_with_citations[p:r])))
                while r < q and not text_with_citations[r].isdigit():
                    r += 1
                p = r
            else:
                break
        p = q + 1
    # Now we have a list of citation locations
    citations.reverse()
    return citations


def replace_citation(
    text_with_citations: str, startpos: int, endpos: int, newValue: int
) -> str:
    """Replaces a citation in a string with a new value.

    Args:
        text_with_citations (str): The text containing the citation to be replaced.
        startpos (int): The starting index of the citation.
        endpos (int): The ending index of the citation.
        newValue (int): The new value to replace the existing citation with.

    Returns:
        str: The updated string with the replaced citation.
    """
    return (
        text_with_citations[0:startpos] + str(newValue) + text_with_citations[endpos:]
    )


def renumber_citations(search_result, offset: int):
    """Renumbers the citations in a search result by a given offset.

    Args:
        search_result (str): The search result text potentially containing citations.
        offset (int): The amount to add to each citation number.

    Returns:
        str: The search result with renumbered citations.
    """
    # Find all citations in the text
    citations = find_citations(search_result)
    searchresponse = search_result
    # Replace all citations with new values
    for cit_start, cit_end, cit_value in citations:
        cit_value += offset
        searchresponse = replace_citation(searchresponse, cit_start, cit_end, cit_value)
    return searchresponse


def insert_citations(response: str, citations: [GroundingCitation]):
    """Inserts citations from a list of GroundingCitation objects into the text.

    Args:
        response (str): The text where citations should be inserted.
        citations (list[GroundingCitation]): A list of GroundingCitation objects from an LLM response.

    Returns:
        str: The text with citations inserted in the form [number] at their corresponding locations.
    """
    cits = [
        {"start": c.start_index, "end": c.end_index, "id": i}
        for i, c in enumerate(citations, start=1)
    ]
    cits.sort(key=lambda x: x["end"], reverse=True)
    for c in cits:
        end = c["end"] + 1
        response = response[:end] + f"[{c['id']}]" + response[end:]
    return response


def get_doc_cit(doc):
    """Extracts the citation number from a document name in the search results.

    Args:
        doc (dict): A document dictionary from the search results, containing a 'name' field.

    Returns:
        int: The citation number extracted from the document name.
    """
    n = doc["name"]
    p = n.find("]")
    return int(n[1:p])


def renumber_docs(search_result, offset: int):
    """Renumbers the citations within document names in a search result.

    Args:
        search_result (dict): The search result potentially containing documents with citations in their names.
        offset (int): The amount to add to each citation number within document names.
    """
    for doc in search_result.get("documents", []):
        n = doc["name"]
        p = n.find("]")
        doc["name"] = f"[{str(int(n[1:p])+offset)}{n[p:]}"


def compact_docs(documents: list[dict], citations: list[tuple]):
    """Compacts the list of documents by removing documents not referenced in the citations.

    Args:
        documents (list[dict]): List of documents from search results, each with a 'name' field containing a citation.
        citations (list[tuple]): List of citations found in the text.

    This function modifies the `documents` list in-place, removing documents that are not cited.
    """
    # Set of unique citation IDs
    cits = set([v for _, _, v in citations])
    for i in range(len(documents) - 1, -1, -1):
        if get_doc_cit(documents[i]) not in cits:
            documents.pop(i)


def extract_article_id(url):
    """Extracts the 8-digit number from a Blick.ch URL.

    Args:
        url (str): The URL string.

    Returns:
        str or None: The extracted 8-digit number as a string, or None if no match is found.
    """
    id_pattern = r"(\d{8})(?=\D|$)"

    # Search for the pattern in the URL and extract ID numbers.
    matched_ids = re.findall(id_pattern, url)

    # Return the first matched ID, or None if no match is found.
    extracted_id = matched_ids[0] if matched_ids else None

    return extracted_id


def date_str_to_unix_int(date_str):
    """Converts a date string in YYYY-MM-DD format to a Unix timestamp integer.

    Args:
        date_str (str): The date string to convert.

    Returns:
        int or None: The Unix timestamp as an integer, or None if the input is invalid.
    """
    try:
        datetime_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
        unix_timestamp = int(datetime_obj.timestamp())
        return unix_timestamp
    except ValueError:
        return None


================================================
File: worker/util/ragas_eval.py
================================================
# Copyright 2024 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This module contains functions for evaluating LLM responses using RAGAS."""

from google.cloud import logging
from datasets import Dataset
from ragas.llms.base import LangchainLLMWrapper
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    answer_similarity,
    context_precision,
    context_recall,
)
from ragas.metrics.critique import harmfulness
from langchain_google_vertexai import VertexAI, VertexAIEmbeddings

from util.settings import settings


# Setup logging
logging_client = logging.Client()
WORKER_LOG_NAME = "litmus-worker-log"
worker_logger = logging_client.logger(WORKER_LOG_NAME)


# Load Gemini Pro model and embeddings for RAGAS
gemini_pro = VertexAI(
    model_name="gemini-1.5-pro", project=settings.project_id, location=settings.location
)
embeddings = VertexAIEmbeddings(
    model_name="textembedding-gecko@003",
    project=settings.project_id,
    location=settings.location,
)

# Compile list of RAGAS Metrics
ragas_metrics = [
    answer_relevancy,
    context_recall,
    context_precision,
    harmfulness,
    answer_similarity,
]


# IMPORTANT: Gemini with RAGAS
# RAGAS is designed to work with OpenAl Models by default. We must set a few attributes to make it work with Gemini
class RAGASVertexAIEmbeddings(VertexAIEmbeddings):
    """Wrapper for RAGAS"""

    async def embed_text(self, text: str) -> list[float]:
        """Embeds a text for semantics similarity"""
        return self.embed([text], 1, "SEMANTIC_SIMILARITY")[0]


# Wrapper to make RAGAS work with Gemini and Vertex AI Embeddings Models
ragas_embeddings = RAGASVertexAIEmbeddings(
    model_name="textembedding-gecko@003",
    project=settings.project_id,
    location=settings.location,
)
ragas_llm = LangchainLLMWrapper(gemini_pro)
for m in ragas_metrics:
    # change LLM for metric
    m.__setattr__("llm", ragas_llm)
    # check if this metric needs embeddings
    if hasattr(m, "embeddings"):
        # if so change with Vertex AI Embeddings
        m.__setattr__("embeddings", ragas_embeddings)


def evaluate_ragas(question, answer, golden_response, context):
    """Evaluates the LLM response using RAGAS metrics.

    Args:
        actual_filtered_response (dict): The filtered response from the LLM.
        output_field (str): The key for the output field in the filtered response.
        golden_response (str): The expected (golden) response.

    Returns:
        dict or str: A dictionary containing the RAGAS evaluation results if successful,
                     or an error message if an exception occurs during evaluation.
    """

    try:
        # Convert to a dataset
        ragas_dataset = Dataset.from_dict(
            {
                "question": [
                    question,
                ],
                "answer": [
                    answer,
                ],
                "contexts": [
                    [
                        context,
                    ],
                ],
                "ground_truth": [
                    golden_response,
                ],
            }
        )

        # Evaluate with RAGAS and store results
        ragas_result = evaluate(
            ragas_dataset,
            metrics=ragas_metrics,
            raise_exceptions=False,
        )

        return ragas_result.to_pandas().to_dict()

    except Exception as e:
        worker_logger.log_text(f"Error in RAGAS evaluation: {str(e)}", severity="ERROR")
        return f"Error during RAGAS evaluation: {str(e)}"


================================================
File: worker/util/settings.py
================================================
# Copyright 2024 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

print("start settings")


class settings:

    # GCP Specific

    project_id = os.environ.get("GCP_PROJECT", "<INSERT-PROJECT>")
    location = os.environ.get("GCP_LOCATION", "us-central1")

    # AI Specific

    ai_location = os.environ.get("AI_LOCATION", "global")
    ai_default_model = os.environ.get("AI_DEFAULT_MODEL", "gemini-1.5-flash-002")
    ai_validation_model = os.environ.get("AI_DEFAULT_MODEL", "gemini-1.5-flash-002")


